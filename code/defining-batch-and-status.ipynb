{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining batch and status\n",
    "\n",
    "For easier manipulation we need to define the batch and the status of the equipment that was at the moment when the sensor value was recorded. For that we need to use values from _BATCH_ and _STATUS_ sensors to be propagated to all other records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2019-01-01.csv', '2019-01-02.csv', '2019-01-03.csv',\n",
       "       '2019-01-04.csv', '2019-01-05.csv', '2019-01-06.csv',\n",
       "       '2019-01-07.csv', '2019-01-08.csv', '2019-01-09.csv',\n",
       "       '2019-01-10.csv', '2019-01-11.csv', '2019-01-12.csv',\n",
       "       '2019-01-13.csv', 'features.csv'], dtype='<U14')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = np.sort([c for c in listdir(join('..','data')) if 'target' not in c and 'csv' in c and 'leftovers' not in c])\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(data_path, leftovers_path=None):\n",
    "    ### import data\n",
    "    \n",
    "    data = pd.read_csv(data_path, parse_dates=['DateTime'])\n",
    "    if leftovers_path:\n",
    "        leftovers = pd.read_csv(leftovers_path, parse_dates=['DateTime'])\n",
    "        data = leftovers.append(data)\n",
    "    \n",
    "    ### split tag to equipment and sensor\n",
    "\n",
    "    data['Equipment'] = data.apply(lambda x: x.TagName.split(\".\")[0], axis=1)\n",
    "    data['Sensor'] = data.apply(lambda x: x.TagName.split(\".\")[1], axis=1)\n",
    "\n",
    "    ### define batch of each record\n",
    "\n",
    "    batches = data[data.Sensor=='BATCH'].sort_values(['Equipment','DateTime']).reset_index()\n",
    "    batches.drop(['Sensor', 'Value', 'TagName'], axis=1, inplace=True)\n",
    "    batches['index'] = np.arange(len(batches))\n",
    "    batches.rename(columns={'DateTime': 'PrevDateTime', 'StringValue': 'Batch'}, inplace=True)\n",
    "\n",
    "    batches_tmp = batches.copy()\n",
    "    batches_tmp.drop('Batch', axis=1, inplace=True)\n",
    "    batches_tmp['index'] -= 1\n",
    "    batches_tmp.rename(columns={'PrevDateTime': 'NextDateTime'}, inplace=True)\n",
    "\n",
    "    batches = batches.merge(batches_tmp, on=['index', 'Equipment']).drop('index', axis=1)\n",
    "    data_with_batch = data.merge(batches) ### ???\n",
    "    data_with_batch = data_with_batch[(data_with_batch.DateTime >= data_with_batch.PrevDateTime)&\n",
    "                                      (data_with_batch.DateTime < data_with_batch.NextDateTime)]\n",
    "    data_with_batch.drop(['PrevDateTime', 'NextDateTime'], axis=1, inplace=True)\n",
    "    del batches, batches_tmp\n",
    "\n",
    "    ### define status of each record\n",
    "\n",
    "    statuses = data[data.Sensor=='STATUS'].sort_values(['Equipment','DateTime']).reset_index()\n",
    "    statuses.drop(['Sensor', 'Value', 'TagName'], axis=1, inplace=True)\n",
    "    statuses['index'] = np.arange(len(statuses))\n",
    "    statuses.rename(columns={'DateTime': 'PrevDateTime', 'StringValue': 'Status'}, inplace=True)\n",
    "\n",
    "    statuses_tmp = statuses.copy()\n",
    "    statuses_tmp.drop('Status', axis=1, inplace=True)\n",
    "    statuses_tmp['index'] -= 1\n",
    "    statuses_tmp.rename(columns={'PrevDateTime': 'NextDateTime'}, inplace=True)\n",
    "\n",
    "    statuses = statuses.merge(statuses_tmp, on=['index', 'Equipment']).drop('index', axis=1)\n",
    "    data_with_status = data_with_batch.merge(statuses)\n",
    "    data_with_status = data_with_status[(data_with_status.DateTime >= data_with_status.PrevDateTime)&\n",
    "                                        (data_with_status.DateTime < data_with_status.NextDateTime)]\n",
    "    data_with_status.drop(['PrevDateTime', 'NextDateTime'], axis=1, inplace=True)\n",
    "    del statuses, statuses_tmp\n",
    "\n",
    "    ### define next time\n",
    "\n",
    "    sensors = data.sort_values(['Equipment','Sensor','DateTime']).reset_index()\n",
    "    sensors.drop(['Value', 'StringValue', 'TagName'], axis=1, inplace=True)\n",
    "    sensors['index'] = np.arange(len(sensors))\n",
    "\n",
    "    sensors_tmp = sensors.copy()\n",
    "    sensors_tmp['index'] -= 1\n",
    "    sensors_tmp.rename(columns={'DateTime': 'NextDateTime'}, inplace=True)\n",
    "\n",
    "    sensors = sensors.merge(sensors_tmp, on=['index', 'Equipment', 'Sensor']).drop('index', axis=1)\n",
    "    sensors['Delta'] = sensors.apply(lambda x: (x.NextDateTime-x.DateTime).seconds, axis=1)\n",
    "\n",
    "    clean_data = data_with_status.merge(sensors)\n",
    "    clean_data.drop(['NextDateTime'], axis=1, inplace=True)\n",
    "    del data_with_batch, data_with_status, sensors\n",
    "\n",
    "    ### produce output\n",
    "\n",
    "    leftovers = data.merge(clean_data[['TagName','DateTime','Batch']], on=['TagName','DateTime'], how='left')\n",
    "    leftovers = leftovers[leftovers.Batch.isna()][['TagName', 'DateTime', 'Value', 'StringValue']]\n",
    "    clean_data = clean_data[['Equipment','Sensor','DateTime','Batch','Status','Value','StringValue','Delta']]\n",
    "\n",
    "    assert leftovers.shape[0] + clean_data.shape[0] == data.shape[0]\n",
    "    leftovers.to_csv(join('..', 'data', files[i][:-4] + '_leftovers.csv'), index=False)\n",
    "    clean_data.to_csv(join('..', 'data', 'processed', files[i]), index=False)\n",
    "    del clean_data, leftovers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will process file by file (day by day). Some batches can start on one day and end only in the next day, that's why after processing one day we will have some leftovers to be used when processing the next day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    data_path = join('..','data',files[i])\n",
    "    if i > 0:\n",
    "        leftovers_path = join('..','data',files[i-1][:-4]+'_leftovers.csv') \n",
    "    else:\n",
    "        leftovers_path = None\n",
    "    process_file(data_path, leftovers_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
